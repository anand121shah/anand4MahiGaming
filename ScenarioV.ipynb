{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787f4e0a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9cef4",
   "metadata": {},
   "source": [
    "## (1) Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0eeb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/train/neg has 12500 entries\n",
      "aclImdb/train/unsup has 50000 entries\n",
      "aclImdb/train/pos has 12500 entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'aclImdb/train/'\n",
    "\n",
    "sub_folders = os.listdir(base_dir)\n",
    "\n",
    "original_directories = []\n",
    "\n",
    "for folder in sub_folders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        all_files_and_dirs = os.listdir(folder_path)\n",
    "        only_files = [f for f in all_files_and_dirs if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        original_directories.append(folder_path)\n",
    "        print(folder_path, 'has', len(only_files), 'entries')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a0adc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aclImdb/train/neg', 'aclImdb/train/unsup', 'aclImdb/train/pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use this list directoies to access and preprocess data\n",
    "original_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff0924",
   "metadata": {},
   "source": [
    "## (2) Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8672162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example review\n",
    "sentence= \"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8525f6",
   "metadata": {},
   "source": [
    "### (a) Removing Punctuations and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcd0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level its better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_clean(text):\n",
    "    \n",
    "    # removing HTML Tags\n",
    "    htmltags = '<.*?>'\n",
    "    text_without_html = re.sub(htmltags, '', text)\n",
    "    \n",
    "    # removing Punctuation\n",
    "    punctuations = string.punctuation\n",
    "    text_without_punct = re.sub(f\"[{punctuations}]\", '', text_without_html)\n",
    "    \n",
    "    #converting texts to lower case\n",
    "    final = text_without_punct.lower()\n",
    "    \n",
    "    return final\n",
    "\n",
    "\n",
    "# sample of function usage\n",
    "sentence = preprocess_clean(sentence)\n",
    "print(\"Cleaned Text:\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8da13b1",
   "metadata": {},
   "source": [
    "### (b) Converting the Sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d431619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words from the Sentence are: ['story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', 'a', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'its', 'singers', 'unfortunately', 'it', 'stays', 'absurd', 'the', 'whole', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', 'the', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'to', 'a', 'third', 'grader', 'on', 'a', 'technical', 'level', 'its', 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'stars', 'sally', 'kirkland', 'and', 'frederic', 'forrest', 'can', 'be', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(cleaned_text):\n",
    "    # Tokenize the sentence\n",
    "    tokenized_words = word_tokenize(cleaned_text)\n",
    "    \n",
    "    return tokenized_words\n",
    "\n",
    "# sample of function usage\n",
    "sentence = tokenize_text(sentence)\n",
    "print(\"Tokenized Words from the Sentence are:\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51d9f8",
   "metadata": {},
   "source": [
    "### (c) Removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64629d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without Stopwords: ['story', 'man', 'unnatural', 'feelings', 'pig', 'starts', 'opening', 'scene', 'terrific', 'example', 'absurd', 'comedy', 'formal', 'orchestra', 'audience', 'turned', 'insane', 'violent', 'mob', 'crazy', 'chantings', 'singers', 'unfortunately', 'stays', 'absurd', 'whole', 'time', 'general', 'narrative', 'eventually', 'making', 'putting', 'even', 'era', 'turned', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'third', 'grader', 'technical', 'level', 'better', 'might', 'think', 'good', 'cinematography', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'stars', 'sally', 'kirkland', 'frederic', 'forrest', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#in order to modify your stop_words set, you can append/edit this list\n",
    "\"\"\"stop_words.append('new words')\"\"\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # initialization of return variable\n",
    "    text_without_stopwords = []\n",
    "    \n",
    "    #iterating thru each word\n",
    "    for word in text:\n",
    "        if word.lower() not in stop_words:\n",
    "            text_without_stopwords.append(word)\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# sample of function usage\n",
    "sentence = remove_stopwords(sentence)\n",
    "print(\"Words without Stopwords:\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a96b9",
   "metadata": {},
   "source": [
    "### (d -1 ) Stemming of the words (this is the tricky part) \n",
    "[ for the sake of accuracy, i avoid this to make sure our dataset stays as rich as possible ]\n",
    "\n",
    "### (d -2 ) Lemmatization of the words\n",
    "[lemmatization is better than stemming as it is dictionary based and this process saves the semantics of words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c15f336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['story', 'man', 'unnatural', 'feeling', 'pig', 'start', 'opening', 'scene', 'terrific', 'example', 'absurd', 'comedy', 'formal', 'orchestra', 'audience', 'turned', 'insane', 'violent', 'mob', 'crazy', 'chanting', 'singer', 'unfortunately', 'stay', 'absurd', 'whole', 'time', 'general', 'narrative', 'eventually', 'making', 'putting', 'even', 'era', 'turned', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'third', 'grader', 'technical', 'level', 'better', 'might', 'think', 'good', 'cinematography', 'future', 'great', 'vilmos', 'zsigmond', 'future', 'star', 'sally', 'kirkland', 'frederic', 'forrest', 'seen', 'briefly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def apply_lemmatization(stemmed_words):\n",
    "    # initialization for return \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    # applying lemmatization\n",
    "    for word in stemmed_words:\n",
    "        lemmatized_word = WordNetLemmatizer().lemmatize(word)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "# sample of function usage\n",
    "sentence = apply_lemmatization(sentence)\n",
    "print(\"Lemmatized Words:\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade4910",
   "metadata": {},
   "source": [
    "### (E) Final Step to perform all on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e81f2f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = preprocess_clean(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    cleaned_tokens = remove_stopwords(tokens)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb720694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the new directories for preprocessed text\n",
    "new_directories = [\n",
    "    'aclImdb/preprocessed_train/neg',\n",
    "    'aclImdb/preprocessed_train/unsup',\n",
    "    'aclImdb/preprocessed_train/pos'\n",
    "]\n",
    "\n",
    "# Create new directories\n",
    "for new_directory in new_directories:\n",
    "    os.makedirs(new_directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e710d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through each original directory\n",
    "# for original_dir, new_dir in zip(original_directories, new_directories):\n",
    "#     for filename in os.listdir(original_dir):\n",
    "#         if filename.endswith('.txt'):\n",
    "#             with open(os.path.join(original_dir, filename), 'r', encoding='utf-8') as f:\n",
    "#                 text = f.read()\n",
    "#                 processed_text = preprocess_text(text)\n",
    "                \n",
    "#                 # Save the processed text into the new directory\n",
    "#                 with open(os.path.join(new_dir, filename), 'w', encoding='utf-8') as f_out:\n",
    "#                     f_out.write(' '.join(processed_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d116e",
   "metadata": {},
   "source": [
    "## (3) Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658f968c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aclImdb/preprocessed_train/neg',\n",
       " 'aclImdb/preprocessed_train/unsup',\n",
       " 'aclImdb/preprocessed_train/pos']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "109faa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./anaconda3/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in ./anaconda3/lib/python3.10/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./anaconda3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in ./anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./anaconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./anaconda3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4adeed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c360d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
