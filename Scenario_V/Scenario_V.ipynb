{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e7CCGqgtcOKgnXbEwpJ_248w58Tdrv8E",
      "authorship_tag": "ABX9TyMVkKaEmmAynvPwYdfhENoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as-bestinclass/anand4MahiGaming/blob/main/Scenario_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependencies"
      ],
      "metadata": {
        "id": "VBbACVk6xLCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "import matplotlib.pyplot as pyplt\n",
        "import seaborn as sborn\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "bH3WyByUxKwe"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Collection and Extraction"
      ],
      "metadata": {
        "id": "PBPloCFHxRzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the online location where the dataset is saved\n",
        "path_of_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "data_directory = \"aclImdb_v1.tar.gz\"\n",
        "\n",
        "# code to write the response form the url into a file and save it\n",
        "returned_response = requests.get(path_of_url)\n",
        "with open(data_directory, 'wb') as file_to_download:\n",
        "    file_to_download.write(returned_response.content)\n",
        "\n",
        "# with the helo of tarfile library, we can extract the file locally in this environment\n",
        "with tarfile.open(data_directory, 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# checking download status\n",
        "if os.path.exists(\"aclImdb\"):\n",
        "    print(\"The Dataset has been successfully downloaded and extracted.\")\n",
        "else:\n",
        "    print(\"Error: Could not locate the downloaded file!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UutsfaKBwEzI",
        "outputId": "b65b14f0-fbdf-4317-aa35-f843ea058aab"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dataset has been successfully downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Pre-Processing"
      ],
      "metadata": {
        "id": "LVkFkpEqxa3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### File and Directory Preprocessor Functions"
      ],
      "metadata": {
        "id": "Z4PBJTy_56qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#processing each text file\n",
        "def file_preprocessor(data):\n",
        "\n",
        "    the_tokens = data.split()\n",
        "\n",
        "    list_stopwords = set(stopwords.words(\"english\")) #selecting the language of the data to preprocess\n",
        "\n",
        "    #Complete Tokenized Words\n",
        "    final_tokens = []\n",
        "    for each in the_tokens:\n",
        "      lower_word = each.lower()\n",
        "      if lower_word not in list_stopwords:\n",
        "        final_tokens.append(each)\n",
        "\n",
        "    #Lemmatizing to preserve context in language\n",
        "    the_lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [the_lemmatizer.lemmatize(word) for word in final_tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "IgMBBBWbY9T4"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing complete directory\n",
        "def directory_preprocessor(directory_path):\n",
        "\n",
        "    data_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    #processing each directory from binary class\n",
        "    for each in [\"pos\", \"neg\"]:\n",
        "        label_directory = os.path.join(directory_path, each)\n",
        "\n",
        "        for filename in os.listdir(label_directory):\n",
        "\n",
        "            with open(os.path.join(label_directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                output_data = file.read()\n",
        "                #after reading each entry in the folder, it calls file_preprocessor to cleanup data\n",
        "                preprocessed_text = file_preprocessor(output_data)\n",
        "                data_list.append(preprocessed_text)\n",
        "                labels_list.append(each)\n",
        "\n",
        "\n",
        "    return data_list, labels_list"
      ],
      "metadata": {
        "id": "7oflTh0aY-zb"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calling To Preprocess"
      ],
      "metadata": {
        "id": "h-LG1nDO6JQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Train Directory\n",
        "train_data, train_labels = directory_preprocessor(\"/content/aclImdb/train\")\n",
        "\n",
        "#Preprocessing Test Directory\n",
        "test_data, test_labels = directory_preprocessor(\"/content/aclImdb/test\")"
      ],
      "metadata": {
        "id": "HnKo3hZ8ZA-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Extraction"
      ],
      "metadata": {
        "id": "B4_EMaVRxlen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Vectorizer Function and number of max features\n",
        "the_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "#vectorized_train_data\n",
        "vectorized_train_features = the_vectorizer.fit_transform(train_data)\n",
        "\n",
        "#vectorized_test_data\n",
        "vectorized_test_features = the_vectorizer.transform(test_data)"
      ],
      "metadata": {
        "id": "5pGx7yUTZG6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Selection"
      ],
      "metadata": {
        "id": "bf8ziqo8xt4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PredictionModel = LogisticRegression()"
      ],
      "metadata": {
        "id": "yUfUw-OWZJMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Training and Evaluation"
      ],
      "metadata": {
        "id": "wnX4vTIDx25-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "UPDgga0_7ui0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model to train the data into an equation/function\n",
        "PredictionModel.fit(vectorized_train_features, train_labels)"
      ],
      "metadata": {
        "id": "LDN0Vdh8x2kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "O-y4JsD67wcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating sample predictions on test data to check quality of newly constructed model\n",
        "predicted_results = PredictionModel.predict(vectorized_test_features)"
      ],
      "metadata": {
        "id": "YCOfLj0gZJGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_accuracy_score = accuracy_score(test_labels, predicted_results)\n",
        "print(\"The Accuracy Score for the generated model is \", the_accuracy_score)"
      ],
      "metadata": {
        "id": "SIWcmvu6_QIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_confusion_matrix = confusion_matrix(test_labels, predicted_results)\n",
        "\n",
        "#plotting heatmap of confusion matrix\n",
        "pyplt.figure(figsize=(5, 5))\n",
        "sborn.heatmap(the_confusion_matrix, annot=True, fmt='g', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "\n",
        "pyplt.xlabel('Predicted Labels')\n",
        "pyplt.ylabel('True Labels')\n",
        "pyplt.title('Confusion Matrix')\n",
        "pyplt.show()"
      ],
      "metadata": {
        "id": "O0ae-v2B_Sis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_classification_report = classification_report(test_labels, predicted_results)\n",
        "print(\"Classification Report:\\n\\n\", the_classification_report)"
      ],
      "metadata": {
        "id": "-v9WHi0C_0Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Interpretation and Improvement"
      ],
      "metadata": {
        "id": "AQHwFTuwyg7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample Data"
      ],
      "metadata": {
        "id": "NdBuhLiCBOxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these are some sample of moview reviews extracted from the website above to demo the model\n",
        "#https://evolutionwriters.com/samples_and_examples/movie_reviews/\n",
        "\n",
        "sample_reviews = [\n",
        "    {\n",
        "        \"title\": \"Forrest Gump\",\n",
        "        \"summary\": \"The film follows the life of Alabama native Forest Gump, a good man with an ideally low IQ of 75, as historical events occur through his eyes. Jenny Curran was one of Forrest Gump’s few childhood friends and his first and only romantic interest. Throughout the film, Gump regularly experiences adversity but never loses his positive outlook. He may have been physically impaired, but he always retained sight of the big picture. He overcame adversity with his mother’s support and received a football scholarship.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Stranger Things\",\n",
        "        \"summary\": \"The following Movie Review On Stranger Things and Its Visual Effects is an important topic for everyone to think about. If you need some great paper writing services to help you craft a similar one, don’t hesitate to address us.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Alice in Wonderland\",\n",
        "        \"summary\": \"Tim Burton’s 'Alice in Wonderland' was probably the most anticipated movie of the year 2010. But is it any good? First of all, I should probably mention that hardcore Lewis Carroll fans may find very disappointing. The movie is merely based on the book. It features most of the same characters and a couple of the same locations, but the story is completely different. Alice is no longer a little girl, but an unconventional young woman who has the same dream of finding herself in Wonderland every night, until one day it happens in real life. Her arrival was expected. Alice is supposed to save the inhabitants from the evil Red Queen and get the reign back to her kind sister, the White Queen. She knows nothing about it, but she is meant to be the knight in shining armor, both figuratively and literally speaking.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Beautiful Mind\",\n",
        "        \"summary\": \"Beautiful Mind is a biographical movie about John Forbes Nash junior, mathematical genius with hard fate. At the beginning of his career he has made enormous contribution in the field of Game Theory, which was a revolution in this mathematical area and almost brought international fame to the author. The movie was filmed in 2001 by Ron Howard and today it is called modern classics: a true drama, making you think and evoking emotions.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "w2hOK4_dymeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample Predictions"
      ],
      "metadata": {
        "id": "_0fN0DmcBQya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AnalyzeSentiment(review):\n",
        "    vectorized_data = the_vectorizer.transform([file_preprocessor(review)])\n",
        "    prediction = PredictionModel.predict(vectorized_data)\n",
        "    if prediction== ['pos']:\n",
        "      return('Positive Sentiment')\n",
        "    elif prediction== ['neg']:\n",
        "      return('Negative Sentiment')\n",
        "    else:\n",
        "      return('Sentiment Not Figuredout')\n",
        "\n",
        "#list to store sentiments from prediction\n",
        "predicted_list = []\n",
        "\n",
        "#predicting each review from sample reviews\n",
        "for each in sample_reviews:\n",
        "    summary_text = each['summary']\n",
        "    review_sentiment = AnalyzeSentiment(summary_text)\n",
        "    each['sentiment'] = review_sentiment\n",
        "    predicted_list.append(each)\n",
        "\n",
        "#Print out reviews\n",
        "for each in predicted_list:\n",
        "    print('Movie:', each['title'])\n",
        "    print('Sentiment:', each['sentiment'], '\\n')"
      ],
      "metadata": {
        "id": "q_TxWAutzS93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Suggestions for Improvement"
      ],
      "metadata": {
        "id": "-GCE3cIDFiCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We all know that making models is very computation and resources sensitive task. Due to limited access of computing power, I chose a simple Logistic regression model for binary classification.\n",
        "\n",
        "Additionally, we can tune number of features while performing feature extraction to scale or text data.\n",
        "\n",
        "If I had more computing power, I would chose pre-trained BERT model to perfrom re-training with dataset provided. BERT is a NLP model and it has higher scores and metrics for performance however, it takes significantly more time and computing power than Logistic regression to construct a model"
      ],
      "metadata": {
        "id": "cB638on6FlW-"
      }
    }
  ]
}